{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation des NÅ“uds pour GNN\n",
    "\n",
    "Ce notebook transforme les labels textuels des nÅ“uds en features numÃ©riques pour entraÃ®ner un GNN.\n",
    "\n",
    "## MÃ©thodes implÃ©mentÃ©es:\n",
    "1. **Sentence Transformers** (RECOMMANDÃ‰) - Embeddings sÃ©mantiques\n",
    "2. **TF-IDF + SVD** - Vectorisation classique\n",
    "3. **Features hybrides** - Structure + sÃ©mantique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Charger le dataset\n",
    "with open('mermaid_graphs.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "print(f\"âœ“ Dataset chargÃ©: {len(df)} graphes\")\n",
    "print(f\"Colonnes: {df.columns.tolist()}\")\n",
    "print(f\"\\nStatistiques:\")\n",
    "print(df[['num_nodes', 'num_edges']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire tous les labels uniques\n",
    "all_labels = []\n",
    "for G in df['graph']:\n",
    "    labels = [d.get('label', 'UNKNOWN') for n, d in G.nodes(data=True)]\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "unique_labels = set(all_labels)\n",
    "\n",
    "print(f\"ğŸ“Š Statistiques des labels:\")\n",
    "print(f\"  - Total labels: {len(all_labels):,}\")\n",
    "print(f\"  - Labels uniques: {len(unique_labels):,}\")\n",
    "print(f\"  - Moyenne par graphe: {len(all_labels)/len(df):.1f}\")\n",
    "print(f\"\\nğŸ“ Exemples de labels:\")\n",
    "print(list(unique_labels)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MÃ©thode 1: Sentence Transformers (RECOMMANDÃ‰)\n",
    "\n",
    "Utilise un modÃ¨le prÃ©-entraÃ®nÃ© pour crÃ©er des embeddings sÃ©mantiques de haute qualitÃ©.\n",
    "\n",
    "**Avantages:**\n",
    "- Capture la sÃ©mantique des concepts\n",
    "- GÃ¨re les synonymes et variations\n",
    "- Pas besoin de vocabulaire fixe\n",
    "- Embeddings de dimension fixe (384 par dÃ©faut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation si nÃ©cessaire\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Charger le modÃ¨le (all-MiniLM-L6-v2: rapide et efficace, 384 dimensions)\n",
    "print(\"ğŸ”„ Chargement du modÃ¨le Sentence Transformer...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ“ ModÃ¨le chargÃ©: dimension d'embedding = {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# CrÃ©er un cache des embeddings pour Ã©viter de recalculer\n",
    "label_to_embedding = {}\n",
    "\n",
    "print(\"\\nğŸ”„ GÃ©nÃ©ration des embeddings pour tous les labels uniques...\")\n",
    "unique_labels_list = list(unique_labels)\n",
    "embeddings = model.encode(unique_labels_list, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "# CrÃ©er le cache\n",
    "for label, emb in zip(unique_labels_list, embeddings):\n",
    "    label_to_embedding[label] = emb\n",
    "\n",
    "print(f\"\\nâœ“ {len(label_to_embedding)} embeddings gÃ©nÃ©rÃ©s\")\n",
    "print(f\"Dimension: {embeddings[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_embeddings_to_graph(G: nx.DiGraph, model, cache: dict) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Ajoute les embeddings Sentence Transformer aux nÅ“uds du graphe.\n",
    "    \"\"\"\n",
    "    G_copy = G.copy()\n",
    "    \n",
    "    for node in G_copy.nodes():\n",
    "        label = G_copy.nodes[node].get('label', 'UNKNOWN')\n",
    "        \n",
    "        # Utiliser le cache ou encoder si nouveau\n",
    "        if label in cache:\n",
    "            embedding = cache[label]\n",
    "        else:\n",
    "            embedding = model.encode([label])[0]\n",
    "            cache[label] = embedding\n",
    "        \n",
    "        # Ajouter l'embedding comme feature\n",
    "        G_copy.nodes[node]['features'] = embedding\n",
    "    \n",
    "    return G_copy\n",
    "\n",
    "# Appliquer Ã  tous les graphes\n",
    "print(\"ğŸ”„ Application des embeddings Ã  tous les graphes...\")\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "df['graph_with_embeddings'] = df['graph'].progress_apply(\n",
    "    lambda g: add_sentence_embeddings_to_graph(g, model, label_to_embedding)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Embeddings ajoutÃ©s Ã  tous les graphes!\")\n",
    "print(f\"\\nExemple de nÅ“ud avec features:\")\n",
    "G_example = df['graph_with_embeddings'].iloc[0]\n",
    "node_example = list(G_example.nodes(data=True))[0]\n",
    "print(f\"Node ID: {node_example[0]}\")\n",
    "print(f\"Label: {node_example[1].get('label', 'N/A')}\")\n",
    "print(f\"Features shape: {node_example[1]['features'].shape}\")\n",
    "print(f\"Features preview: {node_example[1]['features'][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MÃ©thode 2: TF-IDF + SVD\n",
    "\n",
    "Approche classique de NLP pour vectoriser du texte.\n",
    "\n",
    "**Avantages:**\n",
    "- Rapide et lÃ©ger\n",
    "- Pas besoin de modÃ¨le externe\n",
    "- Bon pour identifier les termes importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# CrÃ©er le pipeline TF-IDF + SVD\n",
    "print(\"ğŸ”„ CrÃ©ation du pipeline TF-IDF + SVD...\")\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),  # Unigrams et bigrams\n",
    "        min_df=2,  # ApparaÃ®t au moins 2 fois\n",
    "    )),\n",
    "    ('svd', TruncatedSVD(\n",
    "        n_components=128,  # RÃ©duction Ã  128 dimensions\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit sur tous les labels\n",
    "print(\"EntraÃ®nement du pipeline sur tous les labels...\")\n",
    "tfidf_pipeline.fit(all_labels)\n",
    "\n",
    "# CrÃ©er le cache\n",
    "print(\"GÃ©nÃ©ration des embeddings TF-IDF pour les labels uniques...\")\n",
    "label_to_tfidf = {}\n",
    "for label in tqdm(unique_labels, desc=\"TF-IDF\"):\n",
    "    embedding = tfidf_pipeline.transform([label])[0]\n",
    "    label_to_tfidf[label] = embedding\n",
    "\n",
    "print(f\"\\nâœ“ {len(label_to_tfidf)} embeddings TF-IDF gÃ©nÃ©rÃ©s\")\n",
    "print(f\"Dimension: {list(label_to_tfidf.values())[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tfidf_embeddings_to_graph(G: nx.DiGraph, cache: dict, default_dim: int = 128) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Ajoute les embeddings TF-IDF aux nÅ“uds du graphe.\n",
    "    \"\"\"\n",
    "    G_copy = G.copy()\n",
    "    \n",
    "    for node in G_copy.nodes():\n",
    "        label = G_copy.nodes[node].get('label', 'UNKNOWN')\n",
    "        \n",
    "        # Utiliser le cache\n",
    "        if label in cache:\n",
    "            embedding = cache[label]\n",
    "        else:\n",
    "            # Fallback: vecteur zÃ©ro\n",
    "            embedding = np.zeros(default_dim)\n",
    "        \n",
    "        G_copy.nodes[node]['features_tfidf'] = embedding\n",
    "    \n",
    "    return G_copy\n",
    "\n",
    "# Appliquer Ã  tous les graphes\n",
    "print(\"ğŸ”„ Application des embeddings TF-IDF Ã  tous les graphes...\")\n",
    "df['graph_with_tfidf'] = df['graph'].progress_apply(\n",
    "    lambda g: add_tfidf_embeddings_to_graph(g, label_to_tfidf)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Embeddings TF-IDF ajoutÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MÃ©thode 3: Features Hybrides (Structure + SÃ©mantique)\n",
    "\n",
    "Combine les embeddings sÃ©mantiques avec des features structurelles du graphe.\n",
    "\n",
    "**Features structurelles:**\n",
    "- DegrÃ© du nÅ“ud (in/out/total)\n",
    "- PageRank\n",
    "- Betweenness centrality\n",
    "- Clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_structural_features(G: nx.DiGraph) -> dict:\n    \"\"\"\n    Calcule les features structurelles pour chaque nÅ“ud.\n    Returns: {node_id: [feature_vector]}\n    \"\"\"\n    features = {}\n    \n    # GÃ©rer les graphes vides\n    if G.number_of_nodes() == 0:\n        return features\n    \n    # Calculer les mÃ©triques globales\n    try:\n        pagerank = nx.pagerank(G, max_iter=100)\n    except:\n        pagerank = {n: 1.0/max(1, G.number_of_nodes()) for n in G.nodes()}\n    \n    try:\n        betweenness = nx.betweenness_centrality(G)\n    except:\n        betweenness = {n: 0.0 for n in G.nodes()}\n    \n    try:\n        clustering = nx.clustering(G.to_undirected())\n    except:\n        clustering = {n: 0.0 for n in G.nodes()}\n    \n    # Pour chaque nÅ“ud, crÃ©er le vecteur de features\n    for node in G.nodes():\n        feature_vector = [\n            G.in_degree(node),       # DegrÃ© entrant\n            G.out_degree(node),      # DegrÃ© sortant\n            G.degree(node),          # DegrÃ© total\n            pagerank.get(node, 0.0),\n            betweenness.get(node, 0.0),\n            clustering.get(node, 0.0)\n        ]\n        features[node] = np.array(feature_vector, dtype=np.float32)\n    \n    return features\n\n# Test sur un graphe non vide\nnon_empty_graphs = df[df['num_nodes'] > 0]['graph']\nif len(non_empty_graphs) > 0:\n    G_test = non_empty_graphs.iloc[0]\n    struct_features = compute_structural_features(G_test)\n    print(f\"Features structurelles calculÃ©es pour {len(struct_features)} nÅ“uds\")\n    print(f\"Dimension: {list(struct_features.values())[0].shape[0]}\")\n    print(f\"Exemple: {list(struct_features.values())[0]}\")\nelse:\n    print(\"âš ï¸ Aucun graphe non vide trouvÃ© pour le test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler\n\ndef add_hybrid_features(G: nx.DiGraph, embedding_cache: dict, model) -> nx.DiGraph:\n    \"\"\"\n    Combine features structurelles + embeddings sÃ©mantiques.\n    \"\"\"\n    G_copy = G.copy()\n    \n    # GÃ©rer les graphes vides\n    if G_copy.number_of_nodes() == 0:\n        return G_copy\n    \n    # 1. Calculer features structurelles\n    struct_features = compute_structural_features(G_copy)\n    \n    # 2. Normaliser les features structurelles\n    struct_matrix = np.array(list(struct_features.values()))\n    \n    # GÃ©rer le cas d'un seul nÅ“ud ou graphes trÃ¨s petits\n    if struct_matrix.shape[0] <= 1:\n        # Pas de normalisation possible, utiliser les features brutes\n        struct_normalized = struct_matrix\n    else:\n        # Normalisation standard\n        scaler = StandardScaler()\n        struct_normalized = scaler.fit_transform(struct_matrix)\n    \n    # 3. Combiner avec embeddings\n    for idx, node in enumerate(G_copy.nodes()):\n        label = G_copy.nodes[node].get('label', 'UNKNOWN')\n        \n        # Embedding sÃ©mantique\n        if label in embedding_cache:\n            semantic_emb = embedding_cache[label]\n        else:\n            semantic_emb = model.encode([label])[0]\n        \n        # Features structurelles normalisÃ©es\n        structural_emb = struct_normalized[idx]\n        \n        # ConcatÃ©ner\n        hybrid_features = np.concatenate([semantic_emb, structural_emb])\n        \n        G_copy.nodes[node]['features_hybrid'] = hybrid_features\n    \n    return G_copy\n\n# Appliquer Ã  tous les graphes\nprint(\"ğŸ”„ CrÃ©ation des features hybrides pour tous les graphes...\")\ndf['graph_with_hybrid'] = df['graph'].progress_apply(\n    lambda g: add_hybrid_features(g, label_to_embedding, model)\n)\n\nprint(\"\\nâœ“ Features hybrides crÃ©Ã©es!\")\nG_example = df['graph_with_hybrid'].iloc[0]\nnode_example = list(G_example.nodes(data=True))[0]\nprint(f\"\\nDimension features hybrides: {node_example[1]['features_hybrid'].shape[0]}\")\nprint(f\"  = {model.get_sentence_embedding_dimension()} (semantic) + 6 (structural)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des embeddings (t-SNE)\n",
    "\n",
    "Visualiser les embeddings en 2D pour vÃ©rifier la qualitÃ©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prendre un Ã©chantillon de labels\n",
    "sample_labels = list(unique_labels)[:500]  # Limiter pour la visualisation\n",
    "sample_embeddings = np.array([label_to_embedding[l] for l in sample_labels])\n",
    "\n",
    "print(\"ğŸ”„ RÃ©duction dimensionnelle avec t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(sample_embeddings)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, s=50)\n",
    "\n",
    "# Annoter quelques points\n",
    "for i in range(0, len(sample_labels), 25):\n",
    "    plt.annotate(sample_labels[i][:15], \n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.title(\"Visualisation t-SNE des embeddings (Sentence Transformers)\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE dimension 1\")\n",
    "plt.ylabel(\"t-SNE dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Les concepts similaires devraient Ãªtre proches dans l'espace 2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sauvegarder les rÃ©sultats\n",
    "\n",
    "Sauvegarder le DataFrame avec les 3 versions des graphes vectorisÃ©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le DataFrame complet\n",
    "with open('mermaid_graphs_vectorized.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "print(f\"âœ“ Dataset sauvegardÃ©: mermaid_graphs_vectorized.pkl\")\n",
    "print(f\"  - {len(df)} graphes\")\n",
    "print(f\"  - Colonnes: {df.columns.tolist()}\")\n",
    "\n",
    "# Sauvegarder aussi les caches d'embeddings\n",
    "with open('embedding_caches.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'sentence_transformer': label_to_embedding,\n",
    "        'tfidf': label_to_tfidf\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nâœ“ Caches d'embeddings sauvegardÃ©s: embedding_caches.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RÃ©sumÃ© et recommandations\n",
    "\n",
    "### ğŸ“Š Comparaison des mÃ©thodes\n",
    "\n",
    "| MÃ©thode | Dimension | Avantages | InconvÃ©nients |\n",
    "|---------|-----------|-----------|---------------|\n",
    "| **Sentence Transformer** | 384 | âœ… SÃ©mantique riche<br>âœ… Robuste aux variations<br>âœ… PrÃ©-entraÃ®nÃ© | âŒ Plus lourd<br>âŒ NÃ©cessite GPU (optionnel) |\n",
    "| **TF-IDF + SVD** | 128 | âœ… Rapide<br>âœ… LÃ©ger<br>âœ… Pas de dÃ©pendances lourdes | âŒ Moins sÃ©mantique<br>âŒ Sensible au vocabulaire |\n",
    "| **Hybride** | 390 (384+6) | âœ… Meilleur des deux mondes<br>âœ… Info structurelle | âŒ Plus de dimensions<br>âŒ Plus lent |\n",
    "\n",
    "### ğŸ¯ Recommandation finale\n",
    "\n",
    "Pour ton GNN, je recommande d'utiliser les **features hybrides** (`graph_with_hybrid`):\n",
    "\n",
    "**Pourquoi ?**\n",
    "1. Les **embeddings sÃ©mantiques** capturent le sens des concepts (\"Student Centered\", \"Learning\" sont proches)\n",
    "2. Les **features structurelles** donnent des infos sur le rÃ´le du nÅ“ud dans le graphe (hub, feuille, pont)\n",
    "3. Le GNN peut apprendre Ã  combiner ces deux types d'information\n",
    "\n",
    "**Alternative:**\n",
    "Si les graphes sont trÃ¨s petits (<10 nÅ“uds), utiliser juste les **Sentence Transformers** peut suffire.\n",
    "\n",
    "### ğŸš€ Prochaines Ã©tapes\n",
    "\n",
    "1. Convertir les graphes NetworkX en format PyTorch Geometric\n",
    "2. CrÃ©er des DataLoaders pour l'entraÃ®nement\n",
    "3. DÃ©finir l'architecture du GNN (GCN, GAT, GraphSAGE)\n",
    "4. EntraÃ®ner le modÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques finales\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š STATISTIQUES FINALES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nâœ“ Dataset vectorisÃ© avec succÃ¨s!\")\n",
    "print(f\"  - Nombre de graphes: {len(df)}\")\n",
    "print(f\"  - Labels uniques: {len(unique_labels)}\")\n",
    "print(f\"  - Total nÅ“uds: {df['num_nodes'].sum():,}\")\n",
    "print(f\"  - Total arÃªtes: {df['num_edges'].sum():,}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Dimensions des features:\")\n",
    "G_example = df['graph_with_embeddings'].iloc[0]\n",
    "node_data = list(G_example.nodes(data=True))[0][1]\n",
    "print(f\"  - Sentence Transformer: {node_data['features'].shape[0]}\")\n",
    "\n",
    "G_example = df['graph_with_tfidf'].iloc[0]\n",
    "node_data = list(G_example.nodes(data=True))[0][1]\n",
    "print(f\"  - TF-IDF + SVD: {node_data['features_tfidf'].shape[0]}\")\n",
    "\n",
    "G_example = df['graph_with_hybrid'].iloc[0]\n",
    "node_data = list(G_example.nodes(data=True))[0][1]\n",
    "print(f\"  - Hybride: {node_data['features_hybrid'].shape[0]}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Fichiers gÃ©nÃ©rÃ©s:\")\n",
    "print(f\"  - mermaid_graphs_vectorized.pkl (dataset complet)\")\n",
    "print(f\"  - embedding_caches.pkl (caches pour rÃ©utilisation)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Colonnes du DataFrame final:\")\n",
    "print(f\"  {df.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}